"""LiteLLM-based service for calling language models."""

from __future__ import annotations

import asyncio
import time
from dataclasses import dataclass, field
from typing import TYPE_CHECKING

import litellm
import structlog

if TYPE_CHECKING:
    from agentflow.config import Settings

logger = structlog.get_logger(__name__)

# Exceptions that are worth retrying (transient infrastructure issues).
_RETRYABLE_EXCEPTIONS = (
    litellm.RateLimitError,
    litellm.Timeout,
    litellm.ServiceUnavailableError,
    litellm.APIConnectionError,
)

_MAX_RETRIES = 2
_BASE_BACKOFF_SECONDS = 1.0


@dataclass
class LLMResponse:
    """Structured response returned by :class:`LLMService`.

    Attributes:
        content: The text generated by the model. Empty string on error.
        model: The model identifier that produced the response.
        input_tokens: Number of prompt tokens consumed.
        output_tokens: Number of completion tokens generated.
        latency_ms: Wall-clock time of the API call in milliseconds.
        error: Human-readable error description, or ``None`` on success.
    """

    content: str
    model: str
    input_tokens: int
    output_tokens: int
    latency_ms: int
    error: str | None = field(default=None)

    @property
    def success(self) -> bool:
        """Return ``True`` when the call completed without error."""
        return self.error is None


class LLMService:
    """Async service for making LLM calls via LiteLLM.

    Wraps :func:`litellm.acompletion` with:

    * Configurable model defaulting to ``Settings.default_model``.
    * Retry logic (up to two retries) with exponential backoff for transient
      errors such as rate-limits and timeouts.
    * Structured logging via *structlog*.
    * A consistent :class:`LLMResponse` return type — errors are captured in
      the response rather than raised, so callers never need bare ``try``
      blocks around individual completion calls.

    Example::

        service = LLMService.from_settings(get_settings())
        response = await service.completion(
            user_prompt="Summarise this document.",
            system_prompt="You are a helpful assistant.",
        )
        if response.success:
            print(response.content)
    """

    def __init__(self, model: str) -> None:
        """Initialise the LLM service.

        Args:
            model: LiteLLM model string, e.g. ``"gpt-4o"`` or
                ``"anthropic/claude-3-5-sonnet-20241022"``.
        """
        self.model = model
        self._log = logger.bind(service="LLMService", model=model)

    @classmethod
    def from_settings(cls, settings: Settings) -> LLMService:
        """Create an :class:`LLMService` from application settings.

        Args:
            settings: The application :class:`~agentflow.config.Settings` object.

        Returns:
            A configured :class:`LLMService` instance.
        """
        return cls(model=settings.default_model)

    async def completion(
        self,
        user_prompt: str,
        system_prompt: str | None = None,
        temperature: float = 0.7,
        max_tokens: int = 1024,
    ) -> LLMResponse:
        """Call the LLM and return a structured response.

        Retries up to :data:`_MAX_RETRIES` times with exponential backoff on
        transient errors (rate-limits, timeouts, connection failures).
        Non-retryable errors (e.g. authentication failures, bad requests) are
        returned immediately without retrying.

        Args:
            user_prompt: The user-role message sent to the model.
            system_prompt: Optional system-role instructions. Omitted when
                ``None``.
            temperature: Sampling temperature in the range ``[0.0, 2.0]``.
                Defaults to ``0.7``.
            max_tokens: Maximum number of tokens to generate. Defaults to
                ``1024``.

        Returns:
            :class:`LLMResponse` with content, token counts, and latency.
            On error the ``error`` field is populated and ``content`` is an
            empty string.
        """
        messages: list[dict[str, str]] = []
        if system_prompt is not None:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_prompt})

        log = self._log.bind(temperature=temperature, max_tokens=max_tokens)

        last_exc: BaseException | None = None
        start = time.monotonic()  # initialised here so it's always bound

        for attempt in range(_MAX_RETRIES + 1):
            if attempt > 0:
                backoff = _BASE_BACKOFF_SECONDS * (2 ** (attempt - 1))
                log.warning(
                    "llm_retry",
                    attempt=attempt,
                    backoff_seconds=backoff,
                    previous_error=str(last_exc),
                )
                await asyncio.sleep(backoff)

            start = time.monotonic()
            try:
                response = await litellm.acompletion(
                    model=self.model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                )
            except _RETRYABLE_EXCEPTIONS as exc:
                last_exc = exc
                if attempt < _MAX_RETRIES:
                    continue
                # Final attempt exhausted — fall through to error return below.
            except litellm.AuthenticationError as exc:
                # Credentials are wrong; retrying will not help.
                latency_ms = _elapsed_ms(start)
                log.error(
                    "llm_auth_error",
                    error=str(exc),
                    latency_ms=latency_ms,
                )
                return _error_response(self.model, exc, latency_ms)
            except Exception as exc:  # noqa: BLE001
                latency_ms = _elapsed_ms(start)
                log.error(
                    "llm_unexpected_error",
                    error_type=type(exc).__name__,
                    error=str(exc),
                    latency_ms=latency_ms,
                )
                return _error_response(self.model, exc, latency_ms)
            else:
                # Success path.
                latency_ms = _elapsed_ms(start)
                content = response.choices[0].message.content or ""
                usage = response.usage
                input_tokens: int = getattr(usage, "prompt_tokens", 0) or 0
                output_tokens: int = getattr(usage, "completion_tokens", 0) or 0
                response_model: str = response.model or self.model

                log.info(
                    "llm_completion_success",
                    input_tokens=input_tokens,
                    output_tokens=output_tokens,
                    latency_ms=latency_ms,
                )
                return LLMResponse(
                    content=content,
                    model=response_model,
                    input_tokens=input_tokens,
                    output_tokens=output_tokens,
                    latency_ms=latency_ms,
                )

        # Reached only after exhausting all retries on a retryable error.
        latency_ms = _elapsed_ms(start)
        log.error(
            "llm_retries_exhausted",
            error_type=type(last_exc).__name__,
            error=str(last_exc),
            attempts=_MAX_RETRIES + 1,
            latency_ms=latency_ms,
        )
        return _error_response(self.model, last_exc, latency_ms)


# ---------------------------------------------------------------------------
# Private helpers
# ---------------------------------------------------------------------------

def _elapsed_ms(start: float) -> int:
    """Return milliseconds elapsed since *start* (from :func:`time.monotonic`)."""
    return int((time.monotonic() - start) * 1000)


def _error_response(model: str, exc: BaseException | None, latency_ms: int) -> LLMResponse:
    """Build a failed :class:`LLMResponse` from an exception."""
    error_msg = f"{type(exc).__name__}: {exc}" if exc is not None else "Unknown error"
    return LLMResponse(
        content="",
        model=model,
        input_tokens=0,
        output_tokens=0,
        latency_ms=latency_ms,
        error=error_msg,
    )
